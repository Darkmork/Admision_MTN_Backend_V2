groups:
# SLO Hard Thresholds - Critical System Health Alerts
- name: slo_hard_thresholds
  interval: 30s
  rules:
  # High Latency P99 Alert - Critical for User Experience
  - alert: HighLatencyP99
    expr: service:http:p99_latency_seconds > 2
    for: 5m
    labels:
      severity: page
      team: sre
      service: "{{ $labels.app }}"
      runbook_url: "https://runbooks.mtn.cl/alerts/high-latency-p99"
    annotations:
      summary: "P99 latency >2s en {{ $labels.app }}"
      description: "La latencia P99 HTTP de {{ $labels.app }} es {{ $value }}s, superando el SLO de 2s por más de 5 minutos"
      impact: "Usuarios experimentando respuestas lentas en {{ $labels.app }}"
      action: "1) Verificar CPU/memoria del servicio 2) Revisar queries de BD 3) Escalar réplicas si necesario"

  # High Error Rate Alert - Critical for Service Availability
  - alert: HighErrorRate
    expr: service:http:error_rate > 5
    for: 5m
    labels:
      severity: page
      team: sre
      service: "{{ $labels.app }}"
      runbook_url: "https://runbooks.mtn.cl/alerts/high-error-rate"
    annotations:
      summary: "Error rate >5% en {{ $labels.app }}"
      description: "Tasa de errores HTTP 5xx de {{ $labels.app }} es {{ $value }}%, superando SLO de 5% por más de 5 minutos"
      impact: "{{ $value }}% de requests fallando en {{ $labels.app }}"
      action: "1) Revisar logs de errores 2) Verificar conectividad BD/Redis 3) Considerar rollback si deployment reciente"

  # Dead Letter Queue Alert - Critical for Message Processing
  - alert: DLQNotEmpty
    expr: queue:dlq:messages > 0
    for: 5m
    labels:
      severity: page
      team: sre
      component: messaging
      runbook_url: "https://runbooks.mtn.cl/alerts/dlq-not-empty"
    annotations:
      summary: "DLQ con {{ $value }} mensajes en {{ $labels.queue }}"
      description: "Dead Letter Queue {{ $labels.queue }} contiene {{ $value }} mensajes sin procesar"
      impact: "Mensajes críticos no están siendo procesados, puede afectar flujo de postulaciones"
      action: "1) Revisar logs del consumidor 2) Analizar mensajes en DLQ 3) Reenviar mensajes válidos"

  # High Event Backlog Alert - Warning for Queue Management
  - alert: EventBacklogHigh  
    expr: queue:messages:ready{queue=~"admissions|evaluations|notifications"} > 1000
    for: 10m
    labels:
      severity: warn
      team: sre
      component: messaging
      runbook_url: "https://runbooks.mtn.cl/alerts/event-backlog-high"
    annotations:
      summary: "Backlog alto: {{ $value }} mensajes en {{ $labels.queue }}"
      description: "Cola {{ $labels.queue }} tiene {{ $value }} mensajes pendientes por más de 10 minutos"
      impact: "Procesamiento de eventos más lento, posible retraso en notificaciones"
      action: "1) Escalar consumidores 2) Verificar performance BD 3) Revisar rate limiting"

  # Database Connection Pool Exhaustion
  - alert: DatabaseConnectionPoolHigh
    expr: service:db:connection_pool_utilization > 90
    for: 3m
    labels:
      severity: page
      team: sre
      component: database
      runbook_url: "https://runbooks.mtn.cl/alerts/db-connection-pool"
    annotations:
      summary: "Pool de conexiones DB >90% en {{ $labels.app }}"
      description: "Utilización del pool de conexiones: {{ $value }}% en {{ $labels.app }}"
      impact: "Riesgo de timeout en nuevas conexiones de BD"
      action: "1) Revisar queries lentas 2) Aumentar pool size si necesario 3) Verificar connection leaks"

  # JVM Memory High Utilization
  - alert: JVMMemoryHigh
    expr: service:jvm:memory_utilization_percent > 85
    for: 5m
    labels:
      severity: warn
      team: sre
      component: jvm
      runbook_url: "https://runbooks.mtn.cl/alerts/jvm-memory-high"
    annotations:
      summary: "Memoria JVM >85% en {{ $labels.app }}"
      description: "Utilización memoria heap: {{ $value }}% en {{ $labels.app }}"
      impact: "Riesgo de OutOfMemoryError y degradación de performance"
      action: "1) Revisar memory leaks 2) Analizar heap dump 3) Escalar memoria del pod"

# SLO Burn Rate Alerts - Multi-Window, Multi-Burn-Rate
- name: slo_burn_rate_alerts
  interval: 30s
  rules:
  # Fast Burn Rate - Page Immediately (10% error budget in 5 minutes)
  - alert: SLOErrorRateFastBurn
    expr: |
      (
        sum by (app) (rate(http_server_requests_total{status=~"5.."}[5m]))
        /
        sum by (app) (rate(http_server_requests_total[5m]))
      ) > 0.10
    for: 5m
    labels:
      severity: page
      team: sre
      burn_rate: fast
      service: "{{ $labels.app }}"
      runbook_url: "https://runbooks.mtn.cl/alerts/slo-fast-burn"
    annotations:
      summary: "Fast burn SLO (>10% en 5m) error-rate {{ $labels.app }}"
      description: "Error rate {{ $value | humanizePercentage }} en {{ $labels.app }} - consumiendo error budget muy rápido"
      impact: "SLO mensual en riesgo si continúa esta tasa de errores"
      action: "INMEDIATO: 1) Revisar deployment reciente 2) Considerar rollback 3) Escalar respuesta de equipo"

  # Slow Burn Rate - Create Ticket (3% error budget in 1 hour)  
  - alert: SLOErrorRateSlowBurn
    expr: |
      (
        sum by (app) (rate(http_server_requests_total{status=~"5.."}[1h]))
        /
        sum by (app) (rate(http_server_requests_total[1h]))
      ) > 0.03
    for: 2h
    labels:
      severity: ticket
      team: sre
      burn_rate: slow
      service: "{{ $labels.app }}"
      runbook_url: "https://runbooks.mtn.cl/alerts/slo-slow-burn"
    annotations:
      summary: "Slow burn SLO (>3% en 1h) error-rate {{ $labels.app }}"
      description: "Error rate {{ $value | humanizePercentage }} en {{ $labels.app }} por >2h - consumo sostenido error budget"
      impact: "Tendencia que puede afectar SLO mensual si no se corrige"
      action: "1) Investigar causa root 2) Planificar mejoras 3) Crear ticket seguimiento"

  # Latency Fast Burn Rate
  - alert: SLOLatencyFastBurn
    expr: |
      (
        sum by (app) (rate(http_server_requests_total{le="2"}[5m]))
        /
        sum by (app) (rate(http_server_requests_total[5m]))
      ) < 0.90
    for: 5m
    labels:
      severity: page
      team: sre
      burn_rate: fast
      service: "{{ $labels.app }}"
      runbook_url: "https://runbooks.mtn.cl/alerts/slo-latency-fast-burn"
    annotations:
      summary: "Fast burn SLO latencia (<90% requests <2s) {{ $labels.app }}"
      description: "Solo {{ $value | humanizePercentage }} requests <2s en {{ $labels.app }} - SLO latencia en riesgo"
      impact: "Experiencia usuario degradada significativamente"
      action: "INMEDIATO: 1) Verificar BD queries 2) Revisar CPU/memoria 3) Considerar escalar"

# Business Logic Alerts - MTN Specific
- name: business_logic_alerts
  interval: 30s
  rules:
  # Application Submission Failure Rate
  - alert: ApplicationSubmissionFailureHigh
    expr: |
      (1 - (business:application:success_rate / 100)) > 0.05
    for: 10m
    labels:
      severity: warn
      team: product
      component: applications
      runbook_url: "https://runbooks.mtn.cl/alerts/application-submission-failure"
    annotations:
      summary: "Falla >5% en envío postulaciones"
      description: "Tasa de falla en postulaciones: {{ $value | humanizePercentage }} por >10 minutos"
      impact: "Familias no pueden completar proceso de postulación"
      action: "1) Revisar validaciones de formulario 2) Verificar conectividad BD 3) Analizar logs errores"

  # Authentication Failure Rate High
  - alert: AuthenticationFailureHigh
    expr: |
      (1 - (business:auth:success_rate / 100)) > 0.10
    for: 5m
    labels:
      severity: warn
      team: security
      component: authentication
      runbook_url: "https://runbooks.mtn.cl/alerts/auth-failure-high"
    annotations:
      summary: "Falla >10% en autenticación usuarios"
      description: "Tasa de falla autenticación: {{ $value | humanizePercentage }} en los últimos 5 minutos"
      impact: "Usuarios no pueden acceder al sistema"
      action: "1) Verificar servicio autenticación 2) Revisar JWT tokens 3) Analizar intentos sospechosos"

  # Email Notification Delivery Low
  - alert: EmailDeliveryRateLow
    expr: business:notification:delivery_rate < 0.5
    for: 15m
    labels:
      severity: warn
      team: platform
      component: notifications
      runbook_url: "https://runbooks.mtn.cl/alerts/email-delivery-low"
    annotations:
      summary: "Tasa entrega email <0.5 emails/sec"
      description: "Entrega de emails: {{ $value }} emails/sec por >15 minutos"
      impact: "Familias no recibiendo notificaciones importantes"
      action: "1) Verificar servicio SMTP 2) Revisar rate limits 3) Analizar bounces/rechazos"

# Infrastructure Alerts - System Health
- name: infrastructure_alerts
  interval: 30s
  rules:
  # High CPU Usage
  - alert: HighCPUUsage
    expr: |
      (
        rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m])
        /
        container_spec_cpu_quota{container!="POD",container!=""} * container_spec_cpu_period{container!="POD",container!=""}
      ) * 100 > 80
    for: 10m
    labels:
      severity: warn
      team: sre
      component: infrastructure
    annotations:
      summary: "CPU >80% en {{ $labels.pod }}"
      description: "Uso CPU: {{ $value }}% en pod {{ $labels.pod }}"

  # High Memory Usage
  - alert: HighMemoryUsage
    expr: |
      (
        container_memory_working_set_bytes{container!="POD",container!=""}
        /
        container_spec_memory_limit_bytes{container!="POD",container!=""} 
      ) * 100 > 85
    for: 10m
    labels:
      severity: warn
      team: sre
      component: infrastructure
    annotations:
      summary: "Memoria >85% en {{ $labels.pod }}"
      description: "Uso memoria: {{ $value }}% en pod {{ $labels.pod }}"

  # Pod Restart Rate High
  - alert: PodRestartRateHigh
    expr: |
      rate(kube_pod_container_status_restarts_total[15m]) > 0.1
    for: 10m
    labels:
      severity: warn
      team: sre
      component: kubernetes
    annotations:
      summary: "Pods reiniciando frecuentemente: {{ $labels.pod }}"
      description: "Pod {{ $labels.pod }} reiniciando {{ $value }} veces/minuto"

# Kong API Gateway Alerts
- name: kong_gateway_alerts  
  interval: 30s
  rules:
  # Kong High Error Rate
  - alert: KongHighErrorRate
    expr: |
      (
        sum(rate(kong_http_status{code=~"5.."}[5m]))
        /
        sum(rate(kong_http_status[5m]))
      ) > 0.05
    for: 5m
    labels:
      severity: page
      team: sre  
      component: api_gateway
    annotations:
      summary: "Kong API Gateway error rate >5%"
      description: "Error rate en Kong: {{ $value | humanizePercentage }}"

  # Kong High Latency  
  - alert: KongHighLatency
    expr: |
      histogram_quantile(0.99,
        sum(rate(kong_latency_bucket[5m])) by (le)
      ) > 2
    for: 5m
    labels:
      severity: warn
      team: sre
      component: api_gateway
    annotations:
      summary: "Kong P99 latency >2s"
      description: "Latencia P99 Kong: {{ $value }}s"