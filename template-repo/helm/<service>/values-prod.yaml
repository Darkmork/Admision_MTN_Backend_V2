# Production environment values for <service>
# Production-ready configuration with high availability and security

image:
  tag: "prod"
  pullPolicy: IfNotPresent

# Production resources with proper limits
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 512Mi

# High availability with multiple replicas
replicaCount: 3

# Production autoscaling
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 15
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25  # Conservative scale-down
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15

# Conservative canary rollout for production
rollout:
  enabled: true
  strategy: canary
  canary:
    steps:
      # Very conservative canary steps for production
      - setWeight: 5
      - pause: { duration: 5m }
      - analysis:
          templates:
            - templateName: success-rate
            - templateName: latency-p99
            - templateName: cpu-usage
            - templateName: memory-usage
          args:
            - name: service-name
              value: "{{ include \"<service>.fullname\" . }}"
      - setWeight: 10
      - pause: { duration: 10m }
      - analysis:
          templates:
            - templateName: success-rate
            - templateName: latency-p99
            - templateName: cpu-usage
            - templateName: memory-usage
          args:
            - name: service-name
              value: "{{ include \"<service>.fullname\" . }}"
      - setWeight: 25
      - pause: { duration: 15m }
      - analysis:
          templates:
            - templateName: success-rate
            - templateName: latency-p99
            - templateName: cpu-usage
            - templateName: memory-usage
          args:
            - name: service-name
              value: "{{ include \"<service>.fullname\" . }}"
      - setWeight: 50
      - pause: { duration: 20m }
      - analysis:
          templates:
            - templateName: success-rate
            - templateName: latency-p99
            - templateName: cpu-usage
            - templateName: memory-usage
          args:
            - name: service-name
              value: "{{ include \"<service>.fullname\" . }}"
      - setWeight: 100
    analysis:
      successCondition: result[0] >= 0.99  # Very strict success rate
      failureLimit: 1  # Fail fast
      inconclusiveLimit: 2
      interval: 30s
      count: 10
    maxSurge: "25%"
    maxUnavailable: 0  # Zero downtime
    autoRollbackOnFailure: true
  revisionHistoryLimit: 5
  progressDeadlineSeconds: 1200

# Production environment variables
env:
  - name: SPRING_PROFILES_ACTIVE
    value: "kubernetes,prod"
  - name: LOGGING_LEVEL_ROOT
    value: "WARN"
  - name: LOGGING_LEVEL_COM_DESAFIOS_MTN
    value: "INFO"
  - name: JAVA_OPTS
    value: "-Xmx800m -XX:+UseG1GC -XX:+UseContainerSupport -XX:MaxGCPauseMillis=200 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/"
  - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE
    value: "health,info,metrics,prometheus"
  - name: MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS
    value: "when-authorized"
  - name: MANAGEMENT_ENDPOINT_INFO_SHOW_DETAILS
    value: "when-authorized"

# Production ingress with full SSL and security
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "200"
    nginx.ingress.kubernetes.io/rate-limit-burst: "50"
    nginx.ingress.kubernetes.io/limit-connections: "10"
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "30"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "10"
    nginx.ingress.kubernetes.io/enable-modsecurity: "true"
    nginx.ingress.kubernetes.io/modsecurity-snippet: |
      SecRuleEngine On
      SecRequestBodyAccess On
      SecRule ARGS "@detectSQLi" "phase:2,block,msg:'SQL Injection Attack Detected',id:9001,severity:CRITICAL"
  hosts:
    - host: <service>.mtn.cl
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: <service>-prod-tls
      hosts:
        - <service>.mtn.cl

# Production database configuration
configMap:
  enabled: true
  data:
    DATABASE_URL: "jdbc:postgresql://postgresql-prod.mtn-prod.svc.cluster.local:5432/<service>_prod"
    DATABASE_NAME: "<service>_prod"
    RABBITMQ_HOST: "rabbitmq-prod.mtn-prod.svc.cluster.local"
    RABBITMQ_VHOST: "/prod"
    REDIS_HOST: "redis-prod.mtn-prod.svc.cluster.local"
    LOG_LEVEL: "INFO"
    METRICS_EXPORT_ENABLED: "true"
    TRACING_ENABLED: "true"
    JAEGER_ENDPOINT: "http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces"
    CORS_ALLOWED_ORIGINS: "https://*.mtn.cl"
    SESSION_TIMEOUT: "1800"  # 30 minutes
    JWT_EXPIRATION: "3600"   # 1 hour

# Production secrets (use external secrets operator)
secret:
  enabled: true
  data: {}  # Managed by External Secrets Operator from AWS Secrets Manager

# Maximum security for production
podSecurityContext:
  fsGroup: 1001
  runAsNonRoot: true
  runAsUser: 1001
  runAsGroup: 1001
  seccompProfile:
    type: RuntimeDefault
  fsGroupChangePolicy: "OnRootMismatch"

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1001

# Production-grade service configuration
service:
  type: ClusterIP
  port: 8080
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: "30"

# Comprehensive production monitoring
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    scrapeTimeout: 10s
    labels:
      environment: production
      tier: backend
      team: backend
  prometheusRule:
    enabled: true
    namespace: monitoring
    labels:
      environment: production
      severity: critical
    rules:
      - alert: <Service>ProductionDown
        expr: up{job="<service>",namespace="mtn-prod"} == 0
        for: 30s
        labels:
          severity: critical
          service: <service>
          environment: production
          team: backend
        annotations:
          summary: "PRODUCTION DOWN: <service> is unavailable"
          description: "Service has been down for more than 30 seconds"
          runbook_url: "https://runbooks.mtn.cl/<service>/down"
      
      - alert: <Service>ProductionHighErrorRate
        expr: |
          (
            sum(rate(http_server_requests_total{job="<service>",namespace="mtn-prod",status=~"5.."}[2m])) /
            sum(rate(http_server_requests_total{job="<service>",namespace="mtn-prod"}[2m]))
          ) > 0.01
        for: 1m
        labels:
          severity: critical
          service: <service>
          environment: production
          team: backend
        annotations:
          summary: "PRODUCTION ALERT: High error rate"
          description: "Error rate is {{ $value | humanizePercentage }}"
          runbook_url: "https://runbooks.mtn.cl/<service>/high-error-rate"
      
      - alert: <Service>ProductionHighLatency
        expr: |
          histogram_quantile(0.95,
            sum by (le) (rate(http_server_requests_seconds_bucket{job="<service>",namespace="mtn-prod"}[2m]))
          ) > 0.5
        for: 2m
        labels:
          severity: warning
          service: <service>
          environment: production
          team: backend
        annotations:
          summary: "PRODUCTION WARNING: High latency"
          description: "95th percentile latency is {{ $value }}s"
          runbook_url: "https://runbooks.mtn.cl/<service>/high-latency"
      
      - alert: <Service>ProductionHighMemoryUsage
        expr: |
          (
            container_memory_working_set_bytes{pod=~"<service>-.*",namespace="mtn-prod"} /
            container_spec_memory_limit_bytes{pod=~"<service>-.*",namespace="mtn-prod"}
          ) > 0.85
        for: 5m
        labels:
          severity: warning
          service: <service>
          environment: production
        annotations:
          summary: "PRODUCTION WARNING: High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"

# Production node selection (dedicated nodes)
nodeSelector:
  environment: production
  node-type: application
  instance-type: c5.large

# Production tolerations
tolerations:
  - key: "production-workloads"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  - key: "high-performance"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

# Strong anti-affinity for production
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
                - <service>
        topologyKey: kubernetes.io/hostname
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: node-type
          operator: In
          values:
          - application

# Topology spread for better distribution
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: <service>

# Critical PDB settings
podDisruptionBudget:
  enabled: true
  minAvailable: 2  # Always keep at least 2 pods running

# Production health checks
healthChecks:
  enabled: true
  livenessProbe:
    httpGet:
      path: /actuator/health/liveness
      port: 8080
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    httpGet:
      path: /actuator/health/readiness
      port: 8080
    initialDelaySeconds: 15
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1
  startupProbe:
    httpGet:
      path: /actuator/health
      port: 8080
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 40
    successThreshold: 1

# Strict network policies
networkPolicy:
  enabled: true
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: nginx-ingress
      ports:
        - protocol: TCP
          port: 8080
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 8080
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              name: mtn-prod
      ports:
        - protocol: TCP
          port: 5432  # PostgreSQL
        - protocol: TCP
          port: 5672  # RabbitMQ
        - protocol: TCP
          port: 6379  # Redis
    - to: []
      ports:
        - protocol: TCP
          port: 53   # DNS
        - protocol: UDP
          port: 53   # DNS
        - protocol: TCP
          port: 443  # HTTPS

# High priority for production workloads
priorityClassName: "high-priority"

# Longer termination grace period for production
terminationGracePeriodSeconds: 60

# Production build metadata
build:
  sha: "prod-release"
  url: "https://github.com/mtn-org/<service>/actions/runs/prod"
  deploymentId: "prod-release"
  timestamp: ""
  triggeredBy: "release-manager"

# Enable persistence for production data (if needed)
persistence:
  enabled: false  # Enable if service needs persistent storage
  storageClass: "gp3"
  size: 20Gi
  accessModes:
    - ReadWriteOnce

# Common labels for production
commonLabels:
  environment: production
  tier: backend
  version: stable
  team: backend
  criticality: high

commonAnnotations:
  mtn.cl/environment: production
  mtn.cl/managed-by: helm
  mtn.cl/monitoring: enabled
  mtn.cl/backup: enabled
  mtn.cl/security-scan: enabled
  mtn.cl/sla: 99.9

# Disable tests in production deployment
tests:
  enabled: false

# Feature flags for production (conservative)
featureFlags:
  enabled: true
  canaryTraffic: false
  newFeatures: false
  debugMode: false
  rateLimiting: true
  securityHeaders: true

# Service account with minimal permissions
serviceAccount:
  create: true
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/<service>-prod-role
  name: <service>-prod

# RBAC with minimal permissions
rbac:
  create: true
  rules:
    - apiGroups: [""]
      resources: ["configmaps", "secrets"]
      verbs: ["get", "list"]
    - apiGroups: [""]
      resources: ["pods"]
      verbs: ["get", "list"]