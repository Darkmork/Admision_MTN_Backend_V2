# Staging environment values for <service>
# Production-like environment for final testing

image:
  tag: "staging"
  pullPolicy: IfNotPresent

# Production-like resources
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 256Mi

# Multiple replicas for staging
replicaCount: 2

# Enable autoscaling
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Full canary rollout for staging
rollout:
  enabled: true
  canary:
    steps:
      - setWeight: 10
      - pause: { duration: 1m }
      - analysis:
          templates:
            - templateName: success-rate
            - templateName: latency-p99
          args:
            - name: service-name
              value: "{{ include \"<service>.fullname\" . }}"
      - setWeight: 50
      - pause: { duration: 3m }
      - analysis:
          templates:
            - templateName: success-rate
            - templateName: latency-p99
          args:
            - name: service-name
              value: "{{ include \"<service>.fullname\" . }}"
      - setWeight: 100
    analysis:
      successCondition: result[0] >= 0.95
      failureLimit: 2
      inconclusiveLimit: 3
      interval: 10s
      count: 6

# Staging environment variables
env:
  - name: SPRING_PROFILES_ACTIVE
    value: "kubernetes,staging"
  - name: LOGGING_LEVEL_ROOT
    value: "INFO"
  - name: LOGGING_LEVEL_COM_DESAFIOS_MTN
    value: "INFO"
  - name: JAVA_OPTS
    value: "-Xmx400m -XX:+UseG1GC -XX:+UseContainerSupport"
  - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE
    value: "health,info,metrics,prometheus"

# Staging ingress with SSL
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-staging"
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: <service>-staging.mtn.cl
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: <service>-staging-tls
      hosts:
        - <service>-staging.mtn.cl

# Staging database configuration
configMap:
  enabled: true
  data:
    DATABASE_URL: "jdbc:postgresql://postgresql-staging:5432/<service>_staging"
    DATABASE_NAME: "<service>_staging"
    RABBITMQ_HOST: "rabbitmq-staging.mtn-staging.svc.cluster.local"
    RABBITMQ_VHOST: "/staging"
    REDIS_HOST: "redis-staging.mtn-staging.svc.cluster.local"
    LOG_LEVEL: "INFO"
    METRICS_EXPORT_ENABLED: "true"
    TRACING_ENABLED: "true"
    JAEGER_ENDPOINT: "http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces"

# Staging secrets (use external secrets operator)
secret:
  enabled: true
  data: {}  # Managed by External Secrets Operator

# Production-like security
podSecurityContext:
  fsGroup: 1001
  runAsNonRoot: true
  runAsUser: 1001
  runAsGroup: 1001
  seccompProfile:
    type: RuntimeDefault

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false
  runAsNonRoot: true
  runAsUser: 1001

# Enable load balancer service
service:
  type: ClusterIP
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"

# Enable comprehensive monitoring
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    scrapeTimeout: 10s
    labels:
      environment: staging
      tier: backend
  prometheusRule:
    enabled: true
    labels:
      environment: staging
    rules:
      - alert: <Service>StagingHighErrorRate
        expr: |
          (
            sum(rate(http_server_requests_total{job="<service>",namespace="mtn-staging",status=~"5.."}[2m])) /
            sum(rate(http_server_requests_total{job="<service>",namespace="mtn-staging"}[2m]))
          ) > 0.03
        for: 1m
        labels:
          severity: warning
          service: <service>
          environment: staging
        annotations:
          summary: "Staging: High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}"
      
      - alert: <Service>StagingHighLatency
        expr: |
          histogram_quantile(0.95,
            sum by (le) (rate(http_server_requests_seconds_bucket{job="<service>",namespace="mtn-staging"}[2m]))
          ) > 1
        for: 3m
        labels:
          severity: warning
          service: <service>
          environment: staging
        annotations:
          summary: "Staging: High latency detected"
          description: "95th percentile latency is {{ $value }}s"

# Staging node selection
nodeSelector:
  environment: staging
  node-type: application

# Staging tolerations
tolerations:
  - key: "staging-workloads"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

# Anti-affinity for better distribution
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - <service>
          topologyKey: kubernetes.io/hostname

# Enable PDB for high availability
podDisruptionBudget:
  enabled: true
  minAvailable: 1

# Staging health checks (production-like timing)
healthChecks:
  enabled: true
  livenessProbe:
    httpGet:
      path: /actuator/health/liveness
      port: 8080
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  readinessProbe:
    httpGet:
      path: /actuator/health/readiness
      port: 8080
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
  startupProbe:
    httpGet:
      path: /actuator/health
      port: 8080
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30

# Enable network policies for security
networkPolicy:
  enabled: true
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: mtn-staging
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 8080
  egress:
    - to: []  # Allow all egress for now, restrict as needed

# Staging build metadata
build:
  sha: "staging-latest"
  url: "https://github.com/mtn-org/<service>/actions/runs/staging"
  deploymentId: "staging-auto"
  timestamp: ""
  triggeredBy: "ci-cd"

# Enable persistence if needed for staging tests
persistence:
  enabled: false

# Common labels for staging
commonLabels:
  environment: staging
  tier: backend
  version: staging
  team: backend

commonAnnotations:
  mtn.cl/environment: staging
  mtn.cl/managed-by: helm
  mtn.cl/monitoring: enabled
  mtn.cl/backup: enabled

# Enable tests
tests:
  enabled: true
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

# Feature flags for staging
featureFlags:
  enabled: true
  canaryTraffic: true
  newFeatures: true
  debugMode: false